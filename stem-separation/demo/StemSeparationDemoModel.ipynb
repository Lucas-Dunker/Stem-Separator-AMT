{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the initial implementaiton of our stem separator model that we used for our presentation demo. We've altered this code substantially since then, but we're including this in our final submission for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nussl\n",
    "%pip install git+https://github.com/source-separation/tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import data, utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import IPython.display as display\n",
    "from IPython.display import Audio\n",
    "\n",
    "import nussl\n",
    "from nussl.ml.networks.modules import AmplitudeToDB, BatchNorm, RecurrentStack, Embedding\n",
    "from nussl.datasets import transforms as nussl_tfm\n",
    "from ignite.engine import Events\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MUSDB\n",
    "data.prepare_musdb('~/.nussl/tutorial/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_params = nussl.STFTParams(window_length=512, hop_length=128, window_type='sqrt_hann')\n",
    "\n",
    "tfm = nussl_tfm.Compose([\n",
    "    nussl_tfm.SumSources([['bass', 'drums', 'other']]),\n",
    "    nussl_tfm.MagnitudeSpectrumApproximation(),\n",
    "    nussl_tfm.IndexSources('source_magnitudes', 1),\n",
    "    nussl_tfm.ToSeparationModel(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"~/.nussl/tutorial/train\"\n",
    "val_folder = \"~/.nussl/tutorial/valid\"\n",
    "\n",
    "MAX_MIXTURES = int(1e8) # Set to some impossibly high number for on-the-fly mixing.\n",
    "\n",
    "train_data = data.on_the_fly(stft_params, transform=tfm, fg_path=train_folder, num_mixtures=MAX_MIXTURES, coherent_prob=1.0)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, num_workers=1, batch_size=10)\n",
    "\n",
    "val_data = data.on_the_fly(stft_params, transform=tfm, fg_path=val_folder, num_mixtures=10, coherent_prob=1.0)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, num_workers=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_data[0]\n",
    "for key in item:\n",
    "    print(key, type(item[key]), item[key].shape if isinstance(item[key], torch.Tensor) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfm = nussl_tfm.Compose([\n",
    "    nussl_tfm.SumSources( # Only outputting vocals for our model\n",
    "        groupings=[['drums', 'bass', 'other']],\n",
    "        group_names=['accompaniment'],\n",
    "    ),\n",
    "    nussl_tfm.MagnitudeSpectrumApproximation(),\n",
    "])\n",
    "\n",
    "test_folder = \"~/.nussl/tutorial/test\"\n",
    "\n",
    "test_data = data.on_the_fly(stft_params, transform=test_tfm, fg_path=test_folder, num_mixtures=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = test_data[0]\n",
    "for key in item:\n",
    "    print(key, type(item[key]), item[key].shape if isinstance(item[key], np.ndarray) else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemSeparationModel(nn.Module):\n",
    "    def __init__(self, num_features, num_audio_channels, hidden_size,\n",
    "                 num_layers, bidirectional, dropout, num_sources, \n",
    "                activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.verbose = False\n",
    "\n",
    "        self.amplitude_to_db = AmplitudeToDB()\n",
    "        self.input_normalization = BatchNorm(num_features)\n",
    "        self.recurrent_stack = RecurrentStack(\n",
    "            num_features * num_audio_channels, hidden_size, \n",
    "            num_layers, bool(bidirectional), dropout\n",
    "        )\n",
    "        hidden_size = hidden_size * (int(bidirectional) + 1)\n",
    "        self.embedding = Embedding(num_features, hidden_size, \n",
    "                                   num_sources, activation, \n",
    "                                   num_audio_channels)\n",
    "        \n",
    "        self.set_up_config(num_features, num_audio_channels, hidden_size,\n",
    "                 num_layers, bidirectional, dropout, num_sources, \n",
    "                activation)\n",
    "\n",
    "    def set_up_config(self, num_features, num_audio_channels, hidden_size,\n",
    "                 num_layers, bidirectional, dropout, num_sources, \n",
    "                activation='sigmoid'):\n",
    "        modules = {\n",
    "            'model': {\n",
    "                'class': 'StemSeparationModel',\n",
    "                'args': {\n",
    "                    'num_features': num_features,\n",
    "                    'num_audio_channels': num_audio_channels,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'bidirectional': bidirectional,\n",
    "                    'dropout': dropout,\n",
    "                    'num_sources': num_sources,\n",
    "                    'activation': activation,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        connections = [\n",
    "            ['model', ['mix_magnitude']]\n",
    "        ]\n",
    "\n",
    "        for key in ['mask', 'vocal_estimate']:\n",
    "            modules[key] = {'class': 'Alias'}\n",
    "            connections.append([key, [f'model:{key}']])\n",
    "        \n",
    "        output = ['vocal_estimate', 'mask',]\n",
    "        self.config = {\n",
    "            'name': 'StemSeparationModel',\n",
    "            'modules': modules,\n",
    "            'connections': connections,\n",
    "            'output': output,\n",
    "        }\n",
    "        self.metadata = {\n",
    "            'config': self.config,\n",
    "            'nussl_version': '0.0.1',\n",
    "        }\n",
    "\n",
    "    def log(self, s):\n",
    "        if self.verbose:\n",
    "            print(s)\n",
    "\n",
    "    def forward(self, item):\n",
    "        # Get magnitude of mixture signal\n",
    "        mixture_magnitude = item['mix_magnitude']\n",
    "        if mixture_magnitude.dim() == 3:\n",
    "            mixture_magnitude = mixture_magnitude.unsqueeze(0)  # Add a batch dimension to the mixture magnitude if needed\n",
    "        self.log(f\"Shape of mixture_magnitude: {mixture_magnitude.shape}\")\n",
    "\n",
    "        # Convert to log amplitude\n",
    "        mixture_log_amplitude = self.amplitude_to_db(mixture_magnitude)\n",
    "        self.log(f\"Shape after amplitude to db: {mixture_log_amplitude.shape}\")\n",
    "        \n",
    "        # Normalize the data\n",
    "        normalized = self.input_normalization(mixture_log_amplitude)\n",
    "        self.log(f\"Shape after normalization: {normalized.shape}\")\n",
    "\n",
    "        # Pass through LSTM\n",
    "        output = self.recurrent_stack(normalized)\n",
    "        self.log(f\"Shape after LSTM: {output.shape}\")\n",
    "\n",
    "        # Generate mask\n",
    "        mask = self.embedding(output)\n",
    "        self.log(f\"Shape of mask: {mask.shape}\")\n",
    "    \n",
    "        # Apply mask to get estimates\n",
    "        vocals_estimate = mixture_magnitude.unsqueeze(-1) * mask\n",
    "        self.log(f\"Shape of vocals estimate: {vocals_estimate.shape}\")\n",
    "\n",
    "        return {\n",
    "            'mask': mask,\n",
    "            'vocals_estimate': vocals_estimate,\n",
    "        }\n",
    "\n",
    "    def save(self, location, metadata=None, train_data=None, val_data=None, trainer=None):\n",
    "        torch.save(self, location)\n",
    "        return location\n",
    "    \n",
    "    def __repr__(self):\n",
    "        output = super().__repr__()\n",
    "        num_parameters = 0\n",
    "        for p in self.parameters():\n",
    "            if p.requires_grad:\n",
    "                num_parameters += np.cumprod(p.size())[-1]\n",
    "        output += '\\nNumber of parameters: %d' % num_parameters\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = stft_params.window_length // 2 + 1\n",
    "num_audio_channels = 1\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "num_sources = 1\n",
    "activation = 'sigmoid'\n",
    "\n",
    "model = StemSeparationModel(\n",
    "    num_features=num_features,\n",
    "    num_audio_channels=num_audio_channels,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    "    dropout=dropout,\n",
    "    num_sources=num_sources,\n",
    "    activation=activation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 200\n",
    "EPOCH_LENGTH = 10\n",
    "\n",
    "model.verbose = False\n",
    "\n",
    "utils.logger()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nussl.ml.train.loss.L1Loss()\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch) # forward pass\n",
    "    loss = loss_fn(\n",
    "        output['vocals_estimate'],\n",
    "        batch['source_magnitudes']\n",
    "    )\n",
    "    \n",
    "    loss.backward() # backwards + gradient step\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_vals = {\n",
    "        'L1Loss': loss.item(),\n",
    "        'loss': loss.item()\n",
    "    }\n",
    "\n",
    "    progress_bar = engine.state.pbar\n",
    "    progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return loss_vals\n",
    "\n",
    "def val_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        output = model(batch) # forward pass\n",
    "    loss = loss_fn(\n",
    "        output['vocals_estimate'],\n",
    "        batch['source_magnitudes']\n",
    "    )    \n",
    "    loss_vals = {\n",
    "        'L1Loss': loss.item(), \n",
    "        'loss': loss.item()\n",
    "    }\n",
    "    return loss_vals\n",
    "\n",
    "# Create the engines\n",
    "trainer, validator = nussl.ml.train.create_train_and_validation_engines(\n",
    "    train_step, val_step, device=DEVICE\n",
    ")\n",
    "\n",
    "@trainer.on(Events.EPOCH_STARTED)\n",
    "def setup_pbar(engine):\n",
    "    engine.state.pbar = tqdm(\n",
    "        total=EPOCH_LENGTH,\n",
    "        desc=f\"Epoch {engine.state.epoch}/{NUM_EPOCHS}\",\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def update_pbar(engine):\n",
    "    engine.state.pbar.update(1)\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def close_pbar(engine):\n",
    "    engine.state.pbar.close()\n",
    "\n",
    "# We'll save the output relative to this notebook.\n",
    "output_folder = Path('.').absolute()\n",
    "\n",
    "# Adding handlers from nussl that print out details about model training\n",
    "# run the validation step, and save the models.\n",
    "nussl.ml.train.add_stdout_handler(trainer, validator)\n",
    "nussl.ml.train.add_validate_and_checkpoint(output_folder, model, optimizer, train_data, trainer, val_dataloader, validator)\n",
    "\n",
    "trainer.run(\n",
    "    train_dataloader,\n",
    "    epoch_length=EPOCH_LENGTH,\n",
    "    max_epochs=NUM_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/200-epochs/best.model.pth\"\n",
    "model = torch.load(checkpoint_path, weights_only=False, map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(item, model):    \n",
    "    # Convert mixture signal to tensor if needed\n",
    "    if isinstance(item['mix_magnitude'], np.ndarray):\n",
    "        item['mix_magnitude'] = torch.from_numpy(item['mix_magnitude']).float().to(DEVICE)\n",
    "    \n",
    "    # Transpose for model input\n",
    "    item[\"mix_magnitude\"] = item[\"mix_magnitude\"].transpose(0, 1)\n",
    "    \n",
    "    # Get model output (estimate of vocals source)\n",
    "    with torch.no_grad():\n",
    "        output = model(item)\n",
    "\n",
    "    # Process the vocals estimate\n",
    "    vocals_estimate = output['vocals_estimate']\n",
    "    if vocals_estimate.dim() == 5:  # Remove extra dimensions\n",
    "        vocals_estimate = vocals_estimate.squeeze(0).squeeze(-1).squeeze(-1)\n",
    "    vocals_estimate = vocals_estimate.cpu().data.numpy()\n",
    "\n",
    "    # Get the original mixture phase\n",
    "    mix_stft = item['mix'].stft()\n",
    "    mix_phase = np.angle(mix_stft)\n",
    "\n",
    "    # Make sure vocals_estimate matches the original mixture phase\n",
    "    # We want shape to be (freq_bins, time_frames)\n",
    "    vocals_estimate = vocals_estimate.transpose()\n",
    "    \n",
    "    # Match shapes for combining magnitude and phase\n",
    "    if vocals_estimate.shape[-1] == 1:\n",
    "        vocals_estimate = vocals_estimate.squeeze(-1)\n",
    "    if mix_phase.shape[-1] == 1:\n",
    "        mix_phase = mix_phase.squeeze(-1)\n",
    "\n",
    "    # Verify shapes of magnitude and phase match exactly\n",
    "    assert vocals_estimate.shape == mix_phase.shape, f\"Shape mismatch: vocals_estimate {vocals_estimate.shape} vs mix_phase {mix_phase.shape}\"\n",
    "    \n",
    "    # Reconstruct complex STFT\n",
    "    vocals_estimate_stft = vocals_estimate * np.exp(1j * mix_phase)\n",
    "    \n",
    "    # Create new audio signal with the same parameters as the input audio\n",
    "    vocals_estimate_audio = nussl.AudioSignal(\n",
    "        stft=vocals_estimate_stft,\n",
    "        sample_rate=item['mix'].sample_rate,\n",
    "        stft_params=item['mix'].stft_params,\n",
    "    )\n",
    "\n",
    "    # Perform inverse STFT\n",
    "    vocals_estimate_audio.istft()\n",
    "    \n",
    "    # Ensure the output length matches the input exactly\n",
    "    target_length = len(item['mix'].audio_data[0])\n",
    "    current_length = len(vocals_estimate_audio.audio_data[0])\n",
    "    \n",
    "    # Pad the output audio to match the target length, if necessary\n",
    "    if current_length != target_length:\n",
    "        if current_length < target_length:\n",
    "            pad_length = target_length - current_length\n",
    "            vocals_estimate_audio.audio_data = np.pad(\n",
    "                vocals_estimate_audio.audio_data, \n",
    "                ((0, 0), (0, pad_length)), \n",
    "                mode=\"constant\",\n",
    "            )\n",
    "        else:\n",
    "            vocals_estimate_audio.audio_data = vocals_estimate_audio.audio_data[:, :target_length]\n",
    "    \n",
    "    return vocals_estimate_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_WAV_FILE = True\n",
    "TEST_DATA_ITEM_INDEX = 4\n",
    "\n",
    "item = test_data[TEST_DATA_ITEM_INDEX]\n",
    "item['mix_magnitude'] = torch.from_numpy(item['mix_magnitude']).float().to(DEVICE)\n",
    "vocals_estimate_audio = process_audio(item, model)\n",
    "vocals_estimate_audio.embed_audio(display=False)\n",
    "\n",
    "if SAVE_WAV_FILE:\n",
    "    vocals_estimate_audio.write_audio_to_file(f\"outputs/test-{TEST_DATA_ITEM_INDEX}-vocals-estimate.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('true mixture')\n",
    "display.display(Audio(data=item['mix'].audio_data, rate=item['mix'].sample_rate))\n",
    "\n",
    "for stem_label in item['sources'].keys():\n",
    "    print(f\"true {stem_label}\")\n",
    "    audio_player = Audio(data=item['sources'][stem_label].audio_data, rate=item['sources'][stem_label].sample_rate)\n",
    "    display.display(audio_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, item):\n",
    "    vocals_estimate_audio = process_audio(item, model)\n",
    "    estimates = {\n",
    "        \"vocals\": vocals_estimate_audio,\n",
    "        \"accompaniment\": item[\"mix\"] - vocals_estimate_audio,\n",
    "    }\n",
    "\n",
    "    sources = {\n",
    "        \"vocals\": item[\"sources\"][\"vocals\"],\n",
    "        \"accompaniment\": item[\"sources\"][\"accompaniment\"],\n",
    "    }\n",
    "    source_keys = list(sources.keys())\n",
    "\n",
    "    estimates = [estimates[k] for k in source_keys]\n",
    "    sources = [sources[k] for k in source_keys]\n",
    "\n",
    "    evaluator = nussl.evaluation.BSSEvalScale(\n",
    "        sources, estimates, source_labels=source_keys\n",
    "    )\n",
    "    scores = evaluator.evaluate()\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\"SI-SDR\", \"SI-SIR\", \"SI-SAR\", \"SD-SDR\", \"SNR\", \"SRR\", \"SI-SDRi\", \"SD-SDRi\", \"SNRi\", \"MIX-SI-SDR\", \"MIX-SD-SDR\", \"MIX-SNR\"]\n",
    "NUM_EVALUATION_ITEMS = len(test_data)\n",
    "\n",
    "def evaluate_model(model, data, num_items):\n",
    "    vocals_metrics, accompaniment_metrics = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for i in range(num_items):\n",
    "        item = data[i]\n",
    "        scores = get_scores(model, item)\n",
    "\n",
    "        for metric in METRICS:\n",
    "            vocals_metrics[metric].extend(scores[\"vocals\"][metric])\n",
    "            accompaniment_metrics[metric].extend(scores[\"accompaniment\"][metric])\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"Metric\", \"Vocals Mean\", \"Vocals Standard Deviation\", \"Accompaniment Mean\", \"Accompaniment Standard Deviation\"])\n",
    "    for metric in METRICS:\n",
    "        metric_row = {\n",
    "            \"Metric\": metric,\n",
    "            \"Vocals Mean\": np.mean(vocals_metrics[metric]),\n",
    "            \"Vocals Standard Deviation\": np.std(vocals_metrics[metric]),\n",
    "            \"Accompaniment Mean\": np.mean(accompaniment_metrics[metric]),\n",
    "            \"Accompaniment Standard Deviation\": np.std(accompaniment_metrics[metric]),\n",
    "        }\n",
    "        df = pd.concat([pd.DataFrame([metric_row], columns=df.columns), df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "        \n",
    "eval_df = evaluate_model(model, test_data, NUM_EVALUATION_ITEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"checkpoints/eval_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
